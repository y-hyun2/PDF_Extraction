"""
RAG Answer Generator using Multimodal LLM (Gemma-3n-E4B-it).

Logic:
1. Search Vector DB for query.
2. Retrieve Top-K chunks (Markdown + Metadata).
3. Fetch corresponding Page Images.
4. Construct Multimodal Prompt (Text + Images).
5. Generate Answer.

Handling "All different pages":
- We prioritize unique pages from the Top-K results.
- We limit to N images (e.g., 3) to prevent context overflow.
"""

import argparse
import sys
import os
from pathlib import Path
from typing import List, Dict, Optional

from dotenv import load_dotenv
import torch
from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image

# í—ˆê¹…í˜ì´ìŠ¤ ë¹„ê³µê°œ ëª¨ë¸ ì ‘ê·¼ í† í°ì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜¨ë‹¤.
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")

# ë™ì¼ ë””ë ‰í„°ë¦¬ì˜ ê²€ìƒ‰ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì™€ ì¤‘ë³µ ì œê±°ëœ ê²°ê³¼ë¥¼ ì¬í™œìš©í•œë‹¤.
sys.path.append(str(Path(__file__).parent))
from search_vector_db import search_vector_db

# ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œë„ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë‚¨ê²¨ë‘” ë³´ì¡° ë¡œë” í•¨ìˆ˜.
def load_model(model_id: str):
    print(f"ğŸ“¦ Loading Model '{model_id}'...")
    try:
        processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        print("âœ… Model Loaded.")
        return processor, model
    except Exception as e:
        print(f"âŒ Failed to load model: {e}")
        sys.exit(1)

# ë©”íƒ€ë°ì´í„° íŒíŠ¸ë¥¼ ì´ìš©í•´ data/pages_structured ë‚´ í˜ì´ì§€ ì´ë¯¸ì§€ë¥¼ ì°¾ëŠ”ë‹¤.
def get_page_image_path(metadata: Dict, page_no: Optional[int]) -> Optional[Path]:
    """Locate the PNG corresponding to a page by inferring the report folder name."""

    # í•„ìˆ˜ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œí•œë‹¤.
    if page_no is None:
        return None

    base_dir = Path("data/pages_structured")
    if not base_dir.exists():
        return None

    company = metadata.get('company_name') or metadata.get('company') or ''
    report_year = metadata.get('report_year') or metadata.get('year') or ''
    filename = metadata.get('filename') or ''
    direct_report_dir = (
        metadata.get('report_dir')
        or metadata.get('doc_dir')
        or metadata.get('doc_name_hint')
    )

    candidate_dirs: List[str] = []
    seen = set()

    def add_candidate(name: Optional[str]):
        if not name:
            return
        clean = str(name).strip()
        if not clean or clean in seen:
            return
        candidate_dirs.append(clean)
        seen.add(clean)

    # ë©”íƒ€ë°ì´í„°ì— í´ë”ëª…ì´ ëª…ì‹œë¼ ìˆìœ¼ë©´ ìµœìš°ì„ ìœ¼ë¡œ ì‹œë„í•œë‹¤.
    add_candidate(direct_report_dir)

    if filename:
        stem = Path(filename).stem
        add_candidate(stem)
        add_candidate(stem.replace(" ", "_"))
        add_candidate(stem.replace("-", "_"))

    if company and report_year:
        combos = [
            f"{report_year}_{company}_Report",
            f"{company}_{report_year}_Report",
            f"{report_year}_{company}",
            f"{company}_{report_year}",
        ]
        for combo in combos:
            add_candidate(combo)
            add_candidate(combo.replace(" ", "_"))

    # ìœ„ì—ì„œ ìˆ˜ì§‘í•œ í›„ë³´ ë””ë ‰í„°ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê²€ì‚¬í•œë‹¤.
    page_dir_name = f"page_{page_no:04d}"
    for candidate in candidate_dirs:
        page_path = base_dir / candidate / page_dir_name / "page.png"
        if page_path.exists():
            return page_path

    # ë³´ì¡° ìˆ˜ë‹¨: íšŒì‚¬/ì—°ë„ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ í¬í•¨í•œ í´ë”ë¥¼ í›‘ëŠ”ë‹¤.
    company_upper = company.upper()
    year_str = str(report_year)
    for folder in base_dir.iterdir():
        if not folder.is_dir():
            continue
        folder_name_upper = folder.name.upper()
        if company_upper and company_upper not in folder_name_upper:
            continue
        if year_str and year_str not in folder.name:
            continue
        candidate_path = folder / page_dir_name / "page.png"
        if candidate_path.exists():
            return candidate_path

    # ìµœì¢… ìˆ˜ë‹¨: í˜ì´ì§€ í´ë”ê°€ ì¡´ì¬í•˜ëŠ” ëª¨ë“  ê²½ë¡œë¥¼ í™•ì¸í•œë‹¤.
    for folder in base_dir.iterdir():
        if not folder.is_dir():
            continue
        candidate_path = folder / page_dir_name / "page.png"
        if candidate_path.exists():
            return candidate_path

    return None

# ì „ì²´ RAG + VLM íŒŒì´í”„ë¼ì¸ì„ ì¡°ë¦½í•˜ëŠ” ì—”íŠ¸ë¦¬í¬ì¸íŠ¸.
def main():
    # CLI ì¸ìë¥¼ ì •ì˜í•´ ì§ˆì˜/í•„í„° ë°©ì‹ì„ ì œì–´í•œë‹¤.
    parser = argparse.ArgumentParser(description="RAG Answer Generator")
    parser.add_argument("query", type=str, help="Question to ask")
    parser.add_argument("--model", type=str, default="google/gemma-3-4b-it", help="Model ID") 
    parser.add_argument("--top-k", type=int, default=3, help="Number of chunks to retrieve")
    
    # íšŒì‚¬/ì—°ë„ í•„í„° ì¸ì
    parser.add_argument("--company", type=str, default=None, help="Filter by Company Name (e.g. HDEC)")
    parser.add_argument("--year", type=int, default=None, help="Filter by Report Year (e.g. 2023)")
    
    args = parser.parse_args()
    
    # 1ë‹¨ê³„: ê°€ì¥ ê´€ë ¨ ìˆëŠ” í˜ì´ì§€ë¥¼ ë²¡í„° DBì—ì„œ ê²€ìƒ‰í•œë‹¤.
    print(f"ğŸ” Searching for: '{args.query}'")
    if args.company or args.year:
        print(f"   Filters: Company='{args.company}', Year='{args.year}'")

    results = search_vector_db(args.query, top_k=args.top_k)
    
    # í•„ìš” ì‹œ íšŒì‚¬/ì—°ë„ ì¡°ê±´ìœ¼ë¡œ ê²°ê³¼ë¥¼ í•œ ë²ˆ ë” í•„í„°ë§í•œë‹¤.
    company_filter = args.company.lower() if args.company else None
    year_filter = str(args.year) if args.year else None

    if company_filter or year_filter:
        filtered = []
        for res in results:
            meta = res.get('metadata', {})
            company_name = str(meta.get('company_name') or meta.get('company') or '').lower()
            report_year = str(meta.get('report_year') or meta.get('year') or '')

            if company_filter and company_filter not in company_name:
                continue
            if year_filter and year_filter != report_year:
                continue
            filtered.append(res)

        if not filtered:
            print("âš ï¸ í•„í„° ì¡°ê±´ì— ë§ëŠ” ê²°ê³¼ê°€ ì—†ì–´ ì „ì²´ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            results = filtered

    if not results:
        print("Test ended: No results found.")
        return

    # 2ë‹¨ê³„: í˜ì´ì§€ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ë¥¼ ë¬¶ì–´ ë™ê¸°í™”í•œë‹¤.
    unique_pages = {}  # page_key -> {image_path, texts, page_info} êµ¬ì¡°

    for res in results:
        meta = res.get('metadata', {})
        doc_year = meta.get('report_year', 'UnknownYear')
        company = meta.get('company_name', 'UnknownCompany')
        page_no = meta.get('page_no')
        chunk_text = res.get('content', '')

        if page_no is None:
            continue

        page_key = f"{company}_{doc_year}_{page_no}"

        if page_key not in unique_pages:
            doc_name_hint = f"{doc_year}_{company}_Report"
            meta_with_hint = dict(meta)
            meta_with_hint.setdefault('doc_name_hint', doc_name_hint)
            # ê°€ëŠ¥í•œ í•œ ì •í™•í•œ í˜ì´ì§€ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë¶™ì¸ë‹¤.
            img_path = get_page_image_path(meta_with_hint, page_no)

            unique_pages[page_key] = {
                "image_path": img_path,
                "texts": [],
                "page_info": f"{company} {doc_year} Sustainability Report (Page {page_no})"
            }

        if chunk_text:
            unique_pages[page_key]["texts"].append(chunk_text)

    # 3ë‹¨ê³„: ë©€í‹°ëª¨ë‹¬ ì§€ì‹œí˜• ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•œë‹¤.
    print(f"ğŸ“¦ Loading Model '{args.model}'...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            args.model,
            device_map="auto",
            torch_dtype=torch.bfloat16,  # ë” ì•ˆì •ì ì¸ ì¶”ë¡ ì„ ìœ„í•´ bfloat16 ì‚¬ìš©
            trust_remote_code=True,
            token=HF_TOKEN 
        ).eval()
        processor = AutoProcessor.from_pretrained(
            args.model, 
            trust_remote_code=True,
            token=HF_TOKEN
        )
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        if "gated" in str(e).lower() or "401" in str(e):
             print("ğŸ’¡ Tip: Ensure HF_TOKEN is set in .env and you have access to the model.")
        return

    # 4ë‹¨ê³„: ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ì„ì¸ ë©€í‹°ëª¨ë‹¬ ëŒ€í™” í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•œë‹¤.
    
    messages = [
        {
            "role": "system",
            "content": (
                "You are an ESG report analyst. Use only the provided context. "
                "Never hallucinate or fabricate data. Cite page-level evidence explicitly. "
                "When quoting tables or figures, copy the numbers exactly as shown."
            ),
        },
    ]
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ì— í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ë¥¼ ë²ˆê°ˆì•„ ë°°ì¹˜í•œë‹¤.
    user_content = []
    user_content.append({"type": "text", "text": f"Question: {args.query}\n\nContexts:\n"})
    
    images_loaded = []
    
    # VRAM ì ˆì•½ì„ ìœ„í•´ ìƒìœ„ 3ê°œ í˜ì´ì§€ê¹Œì§€ë§Œ ì‚¬ìš©í•œë‹¤.
    for i, (key, data) in enumerate(list(unique_pages.items())[:3]):
        if data["image_path"]:
            user_content.append({"type": "text", "text": f"--- Page Image ({data['page_info']}) ---\n"})
            user_content.append({"type": "image", "image": str(data["image_path"])}) # Processorê°€ ê²½ë¡œ ë¬¸ìì—´ ë˜ëŠ” PIL ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬
            images_loaded.append(data["image_path"])
        
        texts_combined = "\n... \n".join(data["texts"])
        user_content.append({"type": "text", "text": f"\n[Extracted Text for {data['page_info']}]:\n{texts_combined}\n\n"})

    user_content.append({"type": "text", "text": "Answer:"})
    messages.append({"role": "user", "content": user_content})

    # 5ë‹¨ê³„: ì „ì²´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¡°ê±´ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•œë‹¤.
    print("ğŸ¤– Generating Answer...")
    
    # ëª¨ë¸ ì…ë ¥ í…ì„œë¥¼ ì¤€ë¹„í•œë‹¤.
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # PIL ì´ë¯¸ì§€ ê°ì²´ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.
    pil_images = [Image.open(p) for p in images_loaded] if images_loaded else None
    
    inputs = processor(
        text=[text],
        images=pil_images,
        padding=True,
        return_tensors="pt"
    ).to(model.device)

    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=512)
    
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )[0]

    print("\n" + "="*40)
    print("ğŸ“ Answer:")
    print("="*40)
    print(output_text)
    print("="*40)

if __name__ == "__main__":
    main()
