## ğŸ—„ï¸ ë²¡í„°DB êµ¬ì¡°ë„ (ìµœì¢…)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ“š Vector Database                           â”‚
â”‚                   Collection: "esg_documents"                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“¦ Document Structure per Vector:
â”œâ”€â”€ ğŸ†” vector_id               # UUID (auto-generated)
â”œâ”€â”€ ğŸ“Š embedding                # vector[1024] - bge-m3 ê¸°ì¤€
â”œâ”€â”€ ğŸ“ text                     # ì›ë³¸ í…ìŠ¤íŠ¸ (ê²€ìƒ‰ ëŒ€ìƒ)
â””â”€â”€ ğŸ·ï¸  metadata                # ë©”íƒ€ë°ì´í„° (í•„í„°ë§/ì¶”ì ìš©)
    â”œâ”€â”€ source_type             # "page_chunk" / "figure"
    â”œâ”€â”€ doc_id                  # RDB documents.id (FK)
    â”œâ”€â”€ page_id                 # RDB pages.id (FK, nullable)
    â”œâ”€â”€ page_no                 # í˜ì´ì§€ ë²ˆí˜¸ (1, 2, 3...)
    â”œâ”€â”€ chunk_index             # ì²­í¬ ìˆœì„œ (page_chunkì¸ ê²½ìš°)
    â”œâ”€â”€ figure_id               # RDB doc_figures.id (figureì¸ ê²½ìš°)
    â”œâ”€â”€ image_path              # ì´ë¯¸ì§€ ê²½ë¡œ (figureì¸ ê²½ìš°)
    â”œâ”€â”€ company_name            # "ì‚¼ì„±ì „ì", "í˜„ëŒ€ê±´ì„¤"
    â”œâ”€â”€ report_year             # 2023, 2024
    â”œâ”€â”€ filename                # "samsung_esg_2023.pdf"
    â””â”€â”€ created_at              # "2024-01-28T10:00:00Z"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ¯ Vector Types                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Type 1: page_chunk (Phase 1 - í•„ìˆ˜) â­â­â­
â”œâ”€â”€ Source: pages.full_markdown
â”œâ”€â”€ Process: 512 í† í° ë‹¨ìœ„ë¡œ ì²­í‚¹
â”œâ”€â”€ Count: ~500 vectors per document
â””â”€â”€ Example:
    text: "í˜„ëŒ€ê±´ì„¤ì€ 2023ë…„ íƒ„ì†Œë°°ì¶œëŸ‰ ê°ì¶•ì„ ìœ„í•´...
           [í‘œ 1: Scopeë³„ ë°°ì¶œëŸ‰]
           | êµ¬ë¶„ | 2022 | 2023 |
           [image]
           ìœ„ ê·¸ë˜í”„ëŠ” ì¶”ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤..."

    metadata: {
      "source_type": "page_chunk",
      "doc_id": 5,
      "page_id": 42,
      "page_no": 12,
      "chunk_index": 0,
      "company_name": "í˜„ëŒ€ê±´ì„¤",
      "report_year": 2023
    }

Type 2: figure (Phase 2 - ì„ íƒì ) â­
â”œâ”€â”€ Source: doc_figures.description
â”œâ”€â”€ Condition: descriptionì´ ìˆê³  ê¸¸ì´ > 100ì
â”œâ”€â”€ Count: ~15 vectors per document
â””â”€â”€ Example:
    text: "ê·¸ë¦¼ ì¢…ë¥˜: chart
           ìº¡ì…˜: ì¬ìƒì—ë„ˆì§€ ë¹„ìœ¨ ì¶”ì´

           ìƒì„¸ ì„¤ëª…:
           ì´ ì°¨íŠ¸ëŠ” 2020ë…„ë¶€í„° 2023ë…„ê¹Œì§€ ì¬ìƒì—ë„ˆì§€ ì‚¬ìš© ë¹„ìœ¨ì´
           ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤..."

    metadata: {
      "source_type": "figure",
      "doc_id": 5,
      "page_id": 43,
      "page_no": 15,
      "figure_id": 789,
      "image_path": "/figures/figure_005.png",
      "company_name": "í˜„ëŒ€ê±´ì„¤",
      "report_year": 2023
    }

```

---

## ğŸ”§ ì‹¤ì œ êµ¬í˜„ ì½”ë“œ

### **Phase 1: ê¸°ë³¸ êµ¬ì¡° (í•„ìˆ˜)**

```python
from chromadb import Client
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. ì´ˆê¸°í™”
client = Client(Settings(
    persist_directory="./chroma_db",
    anonymized_telemetry=False
))

collection = client.get_or_create_collection(
    name="esg_documents",
    metadata={"hnsw:space": "cosine"}
)

embed_model = SentenceTransformer('BAAI/bge-m3')

splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=50,
    separators=["\n\n", "\n", ". ", " "]
)

# 2. í˜ì´ì§€ í…ìŠ¤íŠ¸ ë²¡í„°í™”
def add_page_to_vector_db(page, doc):
    """pages.full_markdownì„ ì²­í‚¹í•´ì„œ ë²¡í„°í™”"""

    chunks = splitter.split_text(page.full_markdown)

    ids = []
    embeddings = []
    documents = []
    metadatas = []

    for i, chunk in enumerate(chunks):
        ids.append(f"page_{page.id}_chunk_{i}")
        embeddings.append(embed_model.encode(chunk).tolist())
        documents.append(chunk)
        metadatas.append({
            "source_type": "page_chunk",
            "doc_id": doc.id,
            "page_id": page.id,
            "page_no": page.page_no,
            "chunk_index": i,
            "company_name": doc.company_name,
            "report_year": doc.report_year,
            "filename": doc.filename,
            "created_at": datetime.now().isoformat()
        })

    collection.add(
        ids=ids,
        embeddings=embeddings,
        documents=documents,
        metadatas=metadatas
    )

    return len(chunks)

# 3. ë¬¸ì„œ ì „ì²´ ì¶”ê°€
def add_document_to_vector_db(doc_id: int):
    """ë¬¸ì„œ ì „ì²´ë¥¼ ë²¡í„°DBì— ì¶”ê°€"""

    doc = session.query(Document).get(doc_id)
    total_vectors = 0

    for page in doc.pages:
        count = add_page_to_vector_db(page, doc)
        total_vectors += count

    print(f"âœ… {doc.filename}: {total_vectors} vectors ì¶”ê°€ ì™„ë£Œ")
    return total_vectors

```

---

### **Phase 2: ì„ íƒì  ë³´ê°• (ê·¸ë¦¼ ì„¤ëª…)**

```python
def add_figures_to_vector_db(doc_id: int):
    """
    descriptionì´ ìˆëŠ” ê·¸ë¦¼ë§Œ ì„ íƒì ìœ¼ë¡œ ë²¡í„°í™”
    ì¡°ê±´: len(description) > 100
    """

    doc = session.query(Document).get(doc_id)
    figures = session.query(DocFigure).filter(
        DocFigure.doc_id == doc_id,
        DocFigure.description.isnot(None)
    ).all()

    ids = []
    embeddings = []
    documents = []
    metadatas = []

    for figure in figures:
        # ì„¤ëª…ì´ ì¶©ë¶„íˆ ê¸´ ê²½ìš°ë§Œ
        if len(figure.description) < 100:
            continue

        # í…ìŠ¤íŠ¸ êµ¬ì„±
        figure_text = f"""
ê·¸ë¦¼ ì¢…ë¥˜: {figure.figure_type}
ìº¡ì…˜: {figure.caption}

ìƒì„¸ ì„¤ëª…:
{figure.description}

ìœ„ì¹˜: {doc.company_name} {doc.report_year}ë…„ ë³´ê³ ì„œ {figure.page_no}í˜ì´ì§€
"""

        ids.append(f"figure_{figure.id}")
        embeddings.append(embed_model.encode(figure_text).tolist())
        documents.append(figure_text)
        metadatas.append({
            "source_type": "figure",
            "doc_id": doc.id,
            "page_id": figure.page_id,
            "page_no": figure.page_no,
            "figure_id": figure.id,
            "image_path": figure.image_path,
            "company_name": doc.company_name,
            "report_year": doc.report_year,
            "filename": doc.filename,
            "created_at": datetime.now().isoformat()
        })

    if ids:
        collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas
        )
        print(f"âœ… {doc.filename}: {len(ids)} figure vectors ì¶”ê°€")

    return len(ids)

```

---

## ğŸ” ê²€ìƒ‰ í•¨ìˆ˜

```python
def search_esg_documents(
    query: str,
    company_name: str = None,
    report_year: int = None,
    source_type: str = None,
    top_k: int = 5
):
    """
    ë²¡í„° ê²€ìƒ‰ + ë©”íƒ€ë°ì´í„° í•„í„°ë§

    Args:
        query: ê²€ìƒ‰ ì§ˆë¬¸
        company_name: íšŒì‚¬ëª… í•„í„° (optional)
        report_year: ì—°ë„ í•„í„° (optional)
        source_type: "page_chunk" or "figure" (optional)
        top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
    """

    # ì¿¼ë¦¬ ì„ë² ë”©
    query_embedding = embed_model.encode(query).tolist()

    # í•„í„° êµ¬ì„±
    where_filter = {}
    if company_name:
        where_filter["company_name"] = company_name
    if report_year:
        where_filter["report_year"] = report_year
    if source_type:
        where_filter["source_type"] = source_type

    # ê²€ìƒ‰
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k,
        where=where_filter if where_filter else None
    )

    return results

# ì‚¬ìš© ì˜ˆì‹œ
results = search_esg_documents(
    query="Scope 1 ë°°ì¶œëŸ‰ ì¶”ì´",
    company_name="ì‚¼ì„±ì „ì",
    report_year=2023,
    top_k=5
)

for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0])):
    print(f"\nê²°ê³¼ {i+1}:")
    print(f"ì¶œì²˜: {meta['company_name']} {meta['report_year']}ë…„")
    print(f"í˜ì´ì§€: {meta['page_no']}")
    print(f"íƒ€ì…: {meta['source_type']}")
    print(f"ë‚´ìš©: {doc[:200]}...")

```

---

## ğŸ¯ ì‹¤ì „ ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤

### **ì‹œë‚˜ë¦¬ì˜¤ 1: ì¼ë°˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰**

```python
Q: "ì‚¼ì„±ì „ì 2023ë…„ ì¬ìƒì—ë„ˆì§€ ì‚¬ìš© ë¹„ìœ¨ì€?"

# ê²€ìƒ‰
results = search_esg_documents(
    query="ì¬ìƒì—ë„ˆì§€ ì‚¬ìš© ë¹„ìœ¨",
    company_name="ì‚¼ì„±ì „ì",
    report_year=2023
)

# ê²°ê³¼ (page_chunk)
"""
ìš°ë¦¬ëŠ” ì¬ìƒì—ë„ˆì§€ íˆ¬ìë¥¼ ì§€ì† í™•ëŒ€í•˜ê³  ìˆìŠµë‹ˆë‹¤.
ì•„ë˜ ê·¸ë˜í”„ëŠ” 2020ë…„ë¶€í„° 2023ë…„ê¹Œì§€ì˜ ì¶”ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

[image]

2023ë…„ì—ëŠ” ì „ì²´ ì—ë„ˆì§€ì˜ 35%ë¥¼ ì¬ìƒì—ë„ˆì§€ë¡œ ì¶©ë‹¹í–ˆìŠµë‹ˆë‹¤.
ì´ëŠ” ì „ë…„ ëŒ€ë¹„ 5% ì¦ê°€í•œ ìˆ˜ì¹˜ì…ë‹ˆë‹¤.
"""

# metadataì—ì„œ page_no í™•ì¸ â†’ RDB ì¡°íšŒ â†’ ì •í™•í•œ í‘œ/ê·¸ë¦¼ ì°¾ê¸°

```

---

### **ì‹œë‚˜ë¦¬ì˜¤ 2: ê·¸ë¦¼ íŠ¹í™” ê²€ìƒ‰** (Phase 2)

```python
Q: "íƒ„ì†Œì¤‘ë¦½ ë¡œë“œë§µ ì°¨íŠ¸ ë³´ì—¬ì¤˜"

# ê²€ìƒ‰ (ê·¸ë¦¼ë§Œ í•„í„°ë§)
results = search_esg_documents(
    query="íƒ„ì†Œì¤‘ë¦½ ë¡œë“œë§µ",
    source_type="figure",  # ê·¸ë¦¼ë§Œ!
    top_k=3
)

# ê²°ê³¼ (figure)
"""
ê·¸ë¦¼ ì¢…ë¥˜: chart
ìº¡ì…˜: 2050 íƒ„ì†Œì¤‘ë¦½ ë¡œë“œë§µ

ìƒì„¸ ì„¤ëª…:
ì´ ì°¨íŠ¸ëŠ” 2030ë…„ê¹Œì§€ 50% ê°ì¶•, 2040ë…„ê¹Œì§€ 75% ê°ì¶•,
2050ë…„ íƒ„ì†Œì¤‘ë¦½ ë‹¬ì„±ì„ ëª©í‘œë¡œ í•˜ëŠ” ë‹¨ê³„ë³„ ê³„íšì„ ë³´ì—¬ì¤ë‹ˆë‹¤...
"""

# metadata['image_path']ë¡œ ì‹¤ì œ ì´ë¯¸ì§€ í‘œì‹œ
â†’ /figures/figure_012.png

```

---

### **ì‹œë‚˜ë¦¬ì˜¤ 3: ì‹œê³„ì—´ ë¹„êµ**

```python
Q: "í˜„ëŒ€ê±´ì„¤ ìµœê·¼ 3ë…„ íƒ„ì†Œë°°ì¶œ ì¶”ì´"

# ì—¬ëŸ¬ ì—°ë„ ê²€ìƒ‰
all_results = []
for year in [2021, 2022, 2023]:
    results = search_esg_documents(
        query="íƒ„ì†Œë°°ì¶œëŸ‰ Scope",
        company_name="í˜„ëŒ€ê±´ì„¤",
        report_year=year,
        top_k=3
    )
    all_results.extend(results['metadatas'][0])

# ê° ì—°ë„ë³„ page_no, doc_id í™•ì¸
# â†’ RDBì—ì„œ ì •í™•í•œ í‘œ ë°ì´í„° ì¶”ì¶œ
# â†’ LLMì—ê²Œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ

```

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

```python
# 1. ì¸ë±ìŠ¤ í¬ê¸° ì¡°ì • (HNSW)
collection = client.create_collection(
    name="esg_documents",
    metadata={
        "hnsw:space": "cosine",
        "hnsw:M": 16,              # ì—°ê²° ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì •í™•, ëŠë¦¼)
        "hnsw:construction_ef": 200 # ì¸ë±ìŠ¤ í’ˆì§ˆ
    }
)

# 2. ë°°ì¹˜ ì²˜ë¦¬
def add_document_batch(doc_ids: list):
    """ì—¬ëŸ¬ ë¬¸ì„œ ë™ì‹œ ì²˜ë¦¬"""
    for doc_id in doc_ids:
        add_document_to_vector_db(doc_id)

# 3. ìºì‹±
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_search(query: str, company: str, year: int):
    return search_esg_documents(query, company, year)

```

---

## âœ… ìµœì¢… ì •ë¦¬

### **Phase 1 êµ¬ì¡° (ì‹œì‘)**

```
Vector DB:
â””â”€â”€ page_chunk (500/doc)
    â”œâ”€â”€ text: full_markdown (í‘œ/ê·¸ë¦¼ í¬í•¨, [image] í‘œì‹œ)
    â””â”€â”€ metadata: doc_id, page_no, company, year

```

**ì¥ì :**

- âœ… êµ¬ì¡° ì‹¬í”Œ
- âœ… ëŒ€ë¶€ë¶„ ê²€ìƒ‰ ì»¤ë²„
- âœ… ë¹ ë¥¸ êµ¬ì¶•

**ê²€ìƒ‰ ì„±ëŠ¥:**

- ì¼ë°˜ ì§ˆë¬¸: 90% ì •í™•ë„
- í‘œ ê²€ìƒ‰: 85% ì •í™•ë„
- ê·¸ë¦¼ ê²€ìƒ‰: 70% ì •í™•ë„ (ë§¥ë½ ì˜ì¡´)

---

### **Phase 2 êµ¬ì¡° (ë³´ê°•)**

```
Vector DB:
â”œâ”€â”€ page_chunk (500/doc)
â””â”€â”€ figure (15/doc)  â† ì¶”ê°€!
    â”œâ”€â”€ text: caption + description
    â””â”€â”€ metadata: figure_id, image_path

```

**ì¶”ê°€ ì‹œì :**

- ê·¸ë¦¼ ê²€ìƒ‰ ì •í™•ë„ ë‚®ì„ ë•Œ
- ë³µì¡í•œ ì°¨íŠ¸ ë§ì„ ë•Œ
- ì´ë¯¸ì§€ ìœ„ì£¼ í˜ì´ì§€ ë§ì„ ë•Œ

**ê²€ìƒ‰ ì„±ëŠ¥:**

- ê·¸ë¦¼ ê²€ìƒ‰: 90% ì •í™•ë„ë¡œ í–¥ìƒ!